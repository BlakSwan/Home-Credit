{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score \n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For Kaggle make the dataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\n",
    "# dataPath = \"/kaggle/input/home-credit-credit-risk-model-stability/\"\n",
    "# For local testing make the dataPath = \"/Users/chrisjackson/Downloads/home-credit-credit-risk-model-stability/\"\n",
    "dataPath = \"/Users/chrisjackson/Downloads/home-credit-credit-risk-model-stability/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df): #Standardize the dtype.\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for col in df.columns:  \n",
    "            if df[col].dtype.name in ['object', 'string']:\n",
    "                df[col] = df[col].astype(\"string\").astype('category')\n",
    "                current_categories = df[col].cat.categories\n",
    "                new_categories = current_categories.to_list() + [\"Unknown\"]\n",
    "                new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
    "                df[col] = df[col].astype(new_dtype)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df): #Change the feature for D to the difference in days from date_decision.\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df): #Remove those with an average is_null exceeding 0.95 and those that do not fall within the range 1 < nunique < 200.\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_agg(depth=0, test_train='train') -> pl.DataFrame:\n",
    "    # List all files in the folder\n",
    "    all_files = os.listdir(dataPath + f\"parquet_files/{test_train}/\")\n",
    "\n",
    "    # Filter files where the first number in the filename is depth\n",
    "    filtered_files = []\n",
    "    for file in all_files:\n",
    "        digit = re.search(r'\\d+', file)\n",
    "        if digit and digit.group() == str(depth):\n",
    "            filtered_files.append(file)\n",
    "    \n",
    "    \n",
    "    # Read the base table\n",
    "    aggregated_df = pl.read_parquet(dataPath + f\"parquet_files/{test_train}/{test_train}_base.parquet\")\n",
    "    # Add the static tables to the aggregated_df, meaning the first number in the file name is 0 and they should be listed in filetred_files\n",
    "    for file in filtered_files:\n",
    "        df = pl.read_parquet(dataPath + f\"parquet_files/{test_train}/{file}\")\n",
    "\n",
    "        # Join the aggregated results to the overall aggregated dataframe using the case_id column as the index\n",
    "        aggregated_df = aggregated_df.join(df, on=\"case_id\", how=\"left\")\n",
    "    \n",
    "        # The columns with right in the name might contain info not in the non-right columns.  We need to combine them into one column\n",
    "        # First we need to find the columns with right in the name\n",
    "        right_columns = [col for col in aggregated_df.columns if \"right\" in col]\n",
    "        # Then we need to find the columns without right in the name\n",
    "        non_right_columns = [col for col in aggregated_df.columns if \"right\" not in col]\n",
    "        # We need to iterate over the right columns and add them to the non-right columns and pick the non-null value\n",
    "        for col in right_columns:\n",
    "            # Find the non-right column that corresponds to the right column\n",
    "            non_right_col = col.replace(\"_right\", \"\")\n",
    "            # Add the non-right column to the aggregated_df if the non-right column is null\n",
    "            aggregated_df = aggregated_df.with_columns(\n",
    "            pl.when(pl.col(non_right_col).is_null())\n",
    "            .then(pl.col(col))\n",
    "            .otherwise(pl.col(non_right_col))\n",
    "            .alias(non_right_col)\n",
    "            )\n",
    "        # We need to drop the right columns\n",
    "        aggregated_df = aggregated_df.drop(right_columns)\n",
    "    \n",
    "    # Process the data with the Pipeline class\n",
    "    aggregated_df = Pipeline.set_table_dtypes(aggregated_df)\n",
    "    aggregated_df = Pipeline.handle_dates(aggregated_df)\n",
    "    aggregated_df = Pipeline.filter_cols(aggregated_df)\n",
    "    \n",
    "    # If the depth is 1, then we are interested in the base tables and the tables with a depth of 1\n",
    "    # if depth == 1:\n",
    "    #     # As a test lets try to count the number of applications for each case_id\n",
    "    #     # Gather the files with a depth of 1\n",
    "    #     depth_1_files = [file for file in filtered_files if file[0] == \"1\"]\n",
    "    #     return aggregated_df\n",
    "\n",
    "    \n",
    "    # # Remove basetable from the list of parquet files\n",
    "    # parquet_files = [file for file in parquet_files if file != \"train_base.parquet\"]\n",
    "    \n",
    "    # # Iterate over each parquet file\n",
    "    # for file in parquet_files:\n",
    "    #     # Read the parquet file\n",
    "    #     df = pl.read_parquet(os.path.join(dataPath, file))\n",
    "        \n",
    "    #     # Check if the file contains the column num_group1 and is a file with a depth of 1\n",
    "    #     if \"num_group1\" in df.columns and \"num_group2\" not in df.columns:\n",
    "\n",
    "    #         # Remove the num_group1 column\n",
    "    #         df = df.drop(\"num_group1\")\n",
    "            # Join the aggregated results to the overall aggregated dataframe using the case_id column as the index\n",
    "    #         aggregated_df = aggregated_df.join(df, on=\"case_id\", how=\"outer\")\n",
    "        \n",
    "    #     # Check if the file contains the column num_group1 and num_group2 and is a file with a depth of 2\n",
    "    #     elif \"num_group1\" in df.columns and \"num_group2\" in df.columns and num_group2 is not None:\n",
    "            # # Check if the aggregated df has the num_group1 and num_group2 columns\n",
    "            # if \"num_group2\" not in aggregated_df.columns:\n",
    "            #     # Add the num_group2 column to the aggregated_df\n",
    "            #     aggregated_df = aggregated_df.join(df, on=\"case_id\", how=\"outer\")\n",
    "            # # Filter for the desired num_group1 and num_group2 values\n",
    "            # df = df.filter(pl.col(\"num_group1\") == num_goup1).filter(pl.col(\"num_group2\") == num_group2)\n",
    "            # # Remove the num_group1 and num_group2 columns\n",
    "            # df = df.drop(\"num_group1\").drop(\"num_group2\")\n",
    "            # # Join the aggregated results to the overall aggregated dataframe using the case_id column as the index\n",
    "            # aggregated_df = aggregated_df.join(df, on=\"case_id\", how=\"outer\")\n",
    "        \n",
    "    return aggregated_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_agg(0, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpd_only(data, test_train):\n",
    "    # Find the columns ending in P as they are the days past due columns\n",
    "    dpd_columns = [col for col in data.columns if col.endswith(\"P\")]\n",
    "\n",
    "    # Lets try making a dataframe of the payment history only and see if we can predict the target, might be a path to ensembling\n",
    "    # Create a dataframe of the payment history\n",
    "    if test_train == \"train\":\n",
    "        payment_history = data.select(dpd_columns + [\"target\"] + [\"case_id\"] + [\"WEEK_NUM\"])\n",
    "    else:\n",
    "        payment_history = data.select(dpd_columns + [\"case_id\"] + [\"WEEK_NUM\"])\n",
    "    # Make a run with data as the payment history only\n",
    "    return payment_history\n",
    "\n",
    "data = dpd_only(data, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    case_ids = data[\"case_id\"].unique().shuffle(seed=1)\n",
    "    case_ids_train, case_ids_valid = train_test_split(case_ids, train_size=0.8, random_state=1)\n",
    "\n",
    "    cols_pred = []\n",
    "    for col in data.columns:\n",
    "        if col[-1].isupper() and col[:-1].islower():\n",
    "            cols_pred.append(col)\n",
    "\n",
    "    def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:\n",
    "        return (\n",
    "            data.filter(pl.col(\"case_id\").is_in(case_ids))[cols_pred].to_pandas(),\n",
    "            data.filter(pl.col(\"case_id\").is_in(case_ids))[\"target\"].to_pandas().ravel()\n",
    "        )\n",
    "    \n",
    "    X_train, y_train = from_polars_to_pandas(case_ids_train)\n",
    "    X_valid, y_valid = from_polars_to_pandas(case_ids_valid)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the split_data function\n",
    "X_train, y_train, X_valid, y_valid = split_data(data)\n",
    "\n",
    "# Create a dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    \n",
    "    param = {\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"rf\"]),\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 60),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n",
    "        \"n_estimators\": 1000,\n",
    "        \"verbose\": -1,\n",
    "        \"feature_pre_filter\": False,  # Explicitly disabling feature pre-filtering\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(\n",
    "        param,\n",
    "        lgb_train,\n",
    "        valid_sets=lgb_valid,\n",
    "        callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
    "    )\n",
    "    \n",
    "    preds = gbm.predict(X_valid)\n",
    "    auc = roc_auc_score(y_valid, preds)\n",
    "\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2*roc_auc_score(x[\"target\"], x[\"score\"])-1).tolist()\n",
    "    \n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a*x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.681786\n",
      "[100]\tvalid_0's auc: 0.687016\n",
      "[150]\tvalid_0's auc: 0.689184\n",
      "[200]\tvalid_0's auc: 0.690174\n",
      "[250]\tvalid_0's auc: 0.690862\n",
      "Early stopping, best iteration is:\n",
      "[276]\tvalid_0's auc: 0.691251\n",
      "Initial trial 0 achieved value: 0.69125065849512\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.693013\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's auc: 0.69419\n",
      "Trial 1 achieved value: 0.6941898892610685 with  0.4234% improvement\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.680595\n",
      "[100]\tvalid_0's auc: 0.68337\n",
      "[150]\tvalid_0's auc: 0.685239\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's auc: 0.68532\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.642878\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.690484\n",
      "[100]\tvalid_0's auc: 0.692973\n",
      "[150]\tvalid_0's auc: 0.694151\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid_0's auc: 0.694225\n",
      "Trial 4 achieved value: 0.6942250605790925 with  0.0051% improvement\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.599808\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.692276\n",
      "[100]\tvalid_0's auc: 0.693889\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's auc: 0.694315\n",
      "Trial 6 achieved value: 0.694315032657927 with  0.0130% improvement\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.599808\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\tvalid_0's auc: 0.692722\n",
      "[100]\tvalid_0's auc: 0.694197\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's auc: 0.694448\n",
      "Trial 8 achieved value: 0.6944479326131449 with  0.0191% improvement\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.684466\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4256\n",
      "[LightGBM] [Info] Number of data points in the train set: 1221327, number of used features: 22\n",
      "[LightGBM] [Info] Start training from score 0.031401\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10, callbacks=[champion_callback])  \n",
    "    \n",
    "# Fit model instance\n",
    "model = lgb.train(study.best_params, lgb_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>actualdpdtolerance_344P</th><th>maxdpdfrom6mto36m_3546853P</th><th>maxdpdlast12m_727P</th><th>maxdpdlast24m_143P</th><th>maxdpdlast3m_392P</th><th>maxdpdlast6m_474P</th><th>maxdpdlast9m_1059P</th><th>maxdpdtolerance_374P</th><th>posfpd10lastmonth_333P</th><th>posfpd30lastmonth_3976960P</th><th>posfstqpd30lastmonth_3976962P</th><th>case_id</th><th>WEEK_NUM</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57543</td><td>92</td></tr><tr><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>57549</td><td>92</td></tr><tr><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57551</td><td>92</td></tr><tr><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57552</td><td>92</td></tr><tr><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57569</td><td>92</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>57630</td><td>92</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>57631</td><td>92</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>57632</td><td>92</td></tr><tr><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57633</td><td>92</td></tr><tr><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>57634</td><td>92</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 13)\n",
       "┌────────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬─────────┬──────────┐\n",
       "│ actualdpdt ┆ maxdpdfrom ┆ maxdpdlas ┆ maxdpdlas ┆ … ┆ posfpd30l ┆ posfstqpd ┆ case_id ┆ WEEK_NUM │\n",
       "│ olerance_3 ┆ 6mto36m_35 ┆ t12m_727P ┆ t24m_143P ┆   ┆ astmonth_ ┆ 30lastmon ┆ ---     ┆ ---      │\n",
       "│ 44P        ┆ 46853P     ┆ ---       ┆ ---       ┆   ┆ 3976960P  ┆ th_397696 ┆ i64     ┆ i64      │\n",
       "│ ---        ┆ ---        ┆ f64       ┆ f64       ┆   ┆ ---       ┆ 2P        ┆         ┆          │\n",
       "│ f64        ┆ f64        ┆           ┆           ┆   ┆ f64       ┆ ---       ┆         ┆          │\n",
       "│            ┆            ┆           ┆           ┆   ┆           ┆ f64       ┆         ┆          │\n",
       "╞════════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═════════╪══════════╡\n",
       "│ null       ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 57543   ┆ 92       │\n",
       "│ null       ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 1.0       ┆ 57549   ┆ 92       │\n",
       "│ null       ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 57551   ┆ 92       │\n",
       "│ null       ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 57552   ┆ 92       │\n",
       "│ null       ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 57569   ┆ 92       │\n",
       "│ null       ┆ null       ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ 57630   ┆ 92       │\n",
       "│ null       ┆ null       ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ 57631   ┆ 92       │\n",
       "│ null       ┆ null       ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ 57632   ┆ 92       │\n",
       "│ 0.0        ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 57633   ┆ 92       │\n",
       "│ null       ┆ 0.0        ┆ 0.0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 57634   ┆ 92       │\n",
       "└────────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴─────────┴──────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57543</th>\n",
       "      <td>0.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57549</th>\n",
       "      <td>0.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57551</th>\n",
       "      <td>0.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57552</th>\n",
       "      <td>0.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57569</th>\n",
       "      <td>0.069724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57630</th>\n",
       "      <td>0.049073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57631</th>\n",
       "      <td>0.049073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57632</th>\n",
       "      <td>0.049073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57633</th>\n",
       "      <td>0.069645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57634</th>\n",
       "      <td>0.069724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "case_id          \n",
       "57543    0.069724\n",
       "57549    0.069724\n",
       "57551    0.069724\n",
       "57552    0.069724\n",
       "57569    0.069724\n",
       "57630    0.049073\n",
       "57631    0.049073\n",
       "57632    0.049073\n",
       "57633    0.069645\n",
       "57634    0.069724"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sample submission file\n",
    "sub_df = pd.read_csv(dataPath + \"sample_submission.csv\")\n",
    "# Load the test data\n",
    "data = data_agg(0, 'test')\n",
    "# Make the data the payment history only\n",
    "data = dpd_only(data, 'test')\n",
    "# Generate the predictions from the test files\n",
    "y_sub_pred = model.predict(data, predict_disable_shape_check=True)\n",
    "# Add the predictions to the submission file\n",
    "sub_df = sub_df.set_index(\"case_id\")\n",
    "sub_df['score'] = y_sub_pred\n",
    "# Save the submission file\n",
    "sub_df.to_csv(\"./submission.csv\")\n",
    "sub_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
